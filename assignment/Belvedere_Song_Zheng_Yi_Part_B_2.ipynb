{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/belvdere/NeuralNetworkAssignment/blob/main/Belvedere_Song_Zheng_Yi_Part_B_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFVxWZGJxprU"
      },
      "source": [
        "# Question B2 (10 marks)\n",
        "In Question B1, we used the Category Embedding model. This creates a feedforward neural network in which the categorical features get learnable embeddings. In this question, we will make use of a library called Pytorch-WideDeep. This library makes it easy to work with multimodal deep-learning problems combining images, text, and tables. We will just be utilizing the deeptabular component of this library through the TabMlp network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EycCozG06Duu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d5d316d-6bc7-4f86-8b31-b4d973c5a5d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-widedeep\n",
            "  Downloading pytorch_widedeep-1.6.3-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas>=1.3.5 in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (2.2.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (1.26.4)\n",
            "Collecting scipy<=1.12.0,>=1.7.3 (from pytorch-widedeep)\n",
            "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (1.5.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (4.3.3)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (3.7.5)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (4.10.0.84)\n",
            "Requirement already satisfied: imutils in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (0.5.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (4.66.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (0.19.1+cu121)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (0.8.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (1.16.0)\n",
            "Collecting torchmetrics (from pytorch-widedeep)\n",
            "  Downloading torchmetrics-1.4.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (16.1.0)\n",
            "Collecting fastparquet>=0.8.1 (from pytorch-widedeep)\n",
            "  Downloading fastparquet-2024.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (4.44.2)\n",
            "Collecting sentence-transformers (from pytorch-widedeep)\n",
            "  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from pytorch-widedeep) (0.2.0)\n",
            "Collecting cramjam>=2.3 (from fastparquet>=0.8.1->pytorch-widedeep)\n",
            "  Downloading cramjam-2.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from fastparquet>=0.8.1->pytorch-widedeep) (2024.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastparquet>=0.8.1->pytorch-widedeep) (24.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->pytorch-widedeep) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->pytorch-widedeep) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.5->pytorch-widedeep) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->pytorch-widedeep) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->pytorch-widedeep) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-widedeep) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-widedeep) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-widedeep) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-widedeep) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-widedeep) (3.1.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.15.0->pytorch-widedeep) (10.4.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pytorch-widedeep) (7.0.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->pytorch-widedeep) (0.24.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->pytorch-widedeep) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->pytorch-widedeep) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->pytorch-widedeep) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->pytorch-widedeep) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->pytorch-widedeep) (0.19.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (0.12.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (2.9.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (71.0.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->pytorch-widedeep) (3.4.1)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics->pytorch-widedeep)\n",
            "  Downloading lightning_utilities-0.11.7-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->pytorch-widedeep) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->pytorch-widedeep) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->pytorch-widedeep) (2.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.5->pytorch-widedeep) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->pytorch-widedeep) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->pytorch-widedeep) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->pytorch-widedeep) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->pytorch-widedeep) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->pytorch-widedeep) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->pytorch-widedeep) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->pytorch-widedeep) (0.19.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->pytorch-widedeep) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->pytorch-widedeep) (1.3.0)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->pytorch-widedeep) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->pytorch-widedeep) (0.1.2)\n",
            "Downloading pytorch_widedeep-1.6.3-py3-none-any.whl (21.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.9/21.9 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastparquet-2024.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.4.2-py3-none-any.whl (869 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.2/869.2 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cramjam-2.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.7-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: scipy, lightning-utilities, cramjam, torchmetrics, fastparquet, sentence-transformers, pytorch-widedeep\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "osqp 0.6.7.post0 requires scipy!=1.12.0,>=0.13.2, but you have scipy 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cramjam-2.8.4 fastparquet-2024.5.0 lightning-utilities-0.11.7 pytorch-widedeep-1.6.3 scipy-1.12.0 sentence-transformers-3.1.1 torchmetrics-1.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-widedeep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lq0elU0J53Yo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a15ca67a-4d57-4892-fb58-227566a68af6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap>:914: ImportWarning: _PyDrive2ImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _PyDriveImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _GenerativeAIImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _OpenCVImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: APICoreClientInfoImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _BokehImportHook.find_spec() not found; falling back to find_module()\n",
            "<frozen importlib._bootstrap>:914: ImportWarning: _AltairImportHook.find_spec() not found; falling back to find_module()\n"
          ]
        }
      ],
      "source": [
        "SEED = 42\n",
        "\n",
        "import os\n",
        "\n",
        "import random\n",
        "random.seed(SEED)\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(SEED)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from pytorch_widedeep.preprocessing import TabPreprocessor\n",
        "from pytorch_widedeep.models import TabMlp, WideDeep\n",
        "from pytorch_widedeep import Trainer\n",
        "from pytorch_widedeep.metrics import R2Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU3xdVpwzuLx"
      },
      "source": [
        "1.Divide the dataset (‘hdb_price_prediction.csv’) into train and test sets by using entries from the year 2020 and before as training data, and entries from 2021 and after as the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_oYG6lNIh7Mp"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('hdb_price_prediction.csv')\n",
        "\n",
        "df = df.drop(['full_address', 'nearest_stn'], axis=1)\n",
        "\n",
        "train_df = df[df.year <= 2020]\n",
        "test_df = df[df.year >= 2021]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gE45_OEoWfhx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_q9PoR50JAA"
      },
      "source": [
        "2.Refer to the documentation of Pytorch-WideDeep and perform the following tasks:\n",
        "https://pytorch-widedeep.readthedocs.io/en/latest/index.html\n",
        "* Use [**TabPreprocessor**](https://pytorch-widedeep.readthedocs.io/en/latest/examples/01_preprocessors_and_utils.html#2-tabpreprocessor) to create the deeptabular component using the continuous\n",
        "features and the categorical features. Use this component to transform the training dataset.\n",
        "* Create the [**TabMlp**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/model_components.html#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp) model with 2 linear layers in the MLP, with 200 and 100 neurons respectively.\n",
        "* Create a [**Trainer**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/trainer.html#pytorch_widedeep.training.Trainer) for the training of the created TabMlp model with the root mean squared error (RMSE) cost function. Train the model for 100 epochs using this trainer, keeping a batch size of 64. (Note: set the *num_workers* parameter to 0.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZBY1iqUXtYWn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a658b1e-cbf2-4c61-cd0d-720820ac0f06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_widedeep/preprocessing/tab_preprocessor.py:360: UserWarning: Continuous columns will not be normalised\n",
            "  warnings.warn(\"Continuous columns will not be normalised\")\n"
          ]
        }
      ],
      "source": [
        "continuous_var = [\"dist_to_nearest_stn\", \"dist_to_dhoby\", \"degree_centrality\", \"eigenvector_centrality\", \"remaining_lease_years\", \"floor_area_sqm\"]\n",
        "categorical_var = [\"month\", \"town\", \"flat_model_type\", \"storey_range\"]\n",
        "\n",
        "tab_preprocessor = TabPreprocessor(\n",
        "    cat_embed_cols=categorical_var,\n",
        "    continuous_cols=continuous_var\n",
        ")\n",
        "\n",
        "X_tab = tab_preprocessor.fit_transform(train_df)\n",
        "\n",
        "tab_mlp = TabMlp(\n",
        "    mlp_hidden_dims=[200, 100],\n",
        "    column_idx=tab_preprocessor.column_idx,\n",
        "    cat_embed_input=tab_preprocessor.cat_embed_input,\n",
        "    continuous_cols=continuous_var\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJKis87aWfhx",
        "outputId": "8fdd8da3-3d32-464e-ff51-474b4f62f29c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 1: 100%|██████████| 1366/1366 [00:29<00:00, 47.06it/s, loss=1.8e+5]\n",
            "epoch 2: 100%|██████████| 1366/1366 [00:18<00:00, 73.91it/s, loss=1.01e+5]\n",
            "epoch 3: 100%|██████████| 1366/1366 [00:18<00:00, 74.98it/s, loss=7.94e+4]\n",
            "epoch 4: 100%|██████████| 1366/1366 [00:18<00:00, 72.31it/s, loss=6.51e+4]\n",
            "epoch 5: 100%|██████████| 1366/1366 [00:20<00:00, 68.25it/s, loss=5.95e+4]\n",
            "epoch 6: 100%|██████████| 1366/1366 [00:20<00:00, 68.05it/s, loss=5.74e+4]\n",
            "epoch 7: 100%|██████████| 1366/1366 [00:18<00:00, 73.38it/s, loss=5.63e+4]\n",
            "epoch 8: 100%|██████████| 1366/1366 [00:18<00:00, 74.25it/s, loss=5.52e+4]\n",
            "epoch 9: 100%|██████████| 1366/1366 [00:19<00:00, 71.57it/s, loss=5.43e+4]\n",
            "epoch 10: 100%|██████████| 1366/1366 [00:19<00:00, 71.86it/s, loss=5.34e+4]\n",
            "epoch 11: 100%|██████████| 1366/1366 [00:18<00:00, 71.97it/s, loss=5.27e+4]\n",
            "epoch 12: 100%|██████████| 1366/1366 [00:18<00:00, 73.25it/s, loss=5.2e+4]\n",
            "epoch 13: 100%|██████████| 1366/1366 [00:20<00:00, 67.97it/s, loss=5.14e+4]\n",
            "epoch 14: 100%|██████████| 1366/1366 [00:18<00:00, 72.35it/s, loss=5.08e+4]\n",
            "epoch 15: 100%|██████████| 1366/1366 [00:19<00:00, 71.02it/s, loss=5.02e+4]\n",
            "epoch 16: 100%|██████████| 1366/1366 [00:19<00:00, 69.49it/s, loss=4.96e+4]\n",
            "epoch 17: 100%|██████████| 1366/1366 [00:19<00:00, 70.09it/s, loss=4.94e+4]\n",
            "epoch 18: 100%|██████████| 1366/1366 [00:19<00:00, 68.76it/s, loss=4.91e+4]\n",
            "epoch 19: 100%|██████████| 1366/1366 [00:19<00:00, 68.76it/s, loss=4.88e+4]\n",
            "epoch 20: 100%|██████████| 1366/1366 [00:19<00:00, 71.39it/s, loss=4.85e+4]\n",
            "epoch 21: 100%|██████████| 1366/1366 [00:19<00:00, 71.81it/s, loss=4.85e+4]\n",
            "epoch 22: 100%|██████████| 1366/1366 [00:19<00:00, 71.02it/s, loss=4.81e+4]\n",
            "epoch 23: 100%|██████████| 1366/1366 [00:19<00:00, 69.12it/s, loss=4.82e+4]\n",
            "epoch 24: 100%|██████████| 1366/1366 [00:19<00:00, 69.67it/s, loss=4.8e+4]\n",
            "epoch 25: 100%|██████████| 1366/1366 [00:19<00:00, 71.31it/s, loss=4.81e+4]\n",
            "epoch 26: 100%|██████████| 1366/1366 [00:18<00:00, 72.38it/s, loss=4.78e+4]\n",
            "epoch 27: 100%|██████████| 1366/1366 [00:18<00:00, 72.40it/s, loss=4.78e+4]\n",
            "epoch 28: 100%|██████████| 1366/1366 [00:22<00:00, 61.58it/s, loss=4.79e+4]\n",
            "epoch 29: 100%|██████████| 1366/1366 [00:19<00:00, 69.28it/s, loss=4.77e+4]\n",
            "epoch 30: 100%|██████████| 1366/1366 [00:18<00:00, 72.09it/s, loss=4.77e+4]\n",
            "epoch 31: 100%|██████████| 1366/1366 [00:18<00:00, 72.72it/s, loss=4.75e+4]\n",
            "epoch 32: 100%|██████████| 1366/1366 [00:19<00:00, 71.03it/s, loss=4.75e+4]\n",
            "epoch 33: 100%|██████████| 1366/1366 [00:18<00:00, 72.51it/s, loss=4.75e+4]\n",
            "epoch 34: 100%|██████████| 1366/1366 [00:19<00:00, 71.02it/s, loss=4.74e+4]\n",
            "epoch 35: 100%|██████████| 1366/1366 [00:19<00:00, 71.76it/s, loss=4.72e+4]\n",
            "epoch 36: 100%|██████████| 1366/1366 [00:18<00:00, 72.76it/s, loss=4.72e+4]\n",
            "epoch 37: 100%|██████████| 1366/1366 [00:19<00:00, 69.54it/s, loss=4.72e+4]\n",
            "epoch 38: 100%|██████████| 1366/1366 [00:19<00:00, 70.55it/s, loss=4.72e+4]\n",
            "epoch 39: 100%|██████████| 1366/1366 [00:19<00:00, 70.63it/s, loss=4.7e+4]\n",
            "epoch 40: 100%|██████████| 1366/1366 [00:19<00:00, 69.37it/s, loss=4.69e+4]\n",
            "epoch 41: 100%|██████████| 1366/1366 [00:19<00:00, 71.52it/s, loss=4.7e+4]\n",
            "epoch 42: 100%|██████████| 1366/1366 [00:19<00:00, 70.93it/s, loss=4.69e+4]\n",
            "epoch 43: 100%|██████████| 1366/1366 [00:19<00:00, 71.48it/s, loss=4.7e+4]\n",
            "epoch 44: 100%|██████████| 1366/1366 [00:19<00:00, 69.17it/s, loss=4.68e+4]\n",
            "epoch 45: 100%|██████████| 1366/1366 [00:20<00:00, 66.10it/s, loss=4.67e+4]\n",
            "epoch 46: 100%|██████████| 1366/1366 [00:19<00:00, 70.57it/s, loss=4.66e+4]\n",
            "epoch 47: 100%|██████████| 1366/1366 [00:19<00:00, 68.36it/s, loss=4.65e+4]\n",
            "epoch 48: 100%|██████████| 1366/1366 [00:19<00:00, 69.30it/s, loss=4.66e+4]\n",
            "epoch 49: 100%|██████████| 1366/1366 [00:19<00:00, 69.82it/s, loss=4.66e+4]\n",
            "epoch 50: 100%|██████████| 1366/1366 [00:19<00:00, 70.48it/s, loss=4.66e+4]\n",
            "epoch 51: 100%|██████████| 1366/1366 [00:19<00:00, 69.57it/s, loss=4.65e+4]\n",
            "epoch 52: 100%|██████████| 1366/1366 [00:20<00:00, 68.06it/s, loss=4.65e+4]\n",
            "epoch 53: 100%|██████████| 1366/1366 [00:20<00:00, 65.96it/s, loss=4.64e+4]\n",
            "epoch 54: 100%|██████████| 1366/1366 [00:20<00:00, 67.82it/s, loss=4.63e+4]\n",
            "epoch 55: 100%|██████████| 1366/1366 [00:19<00:00, 68.72it/s, loss=4.63e+4]\n",
            "epoch 56: 100%|██████████| 1366/1366 [00:19<00:00, 69.12it/s, loss=4.63e+4]\n",
            "epoch 57: 100%|██████████| 1366/1366 [00:19<00:00, 68.31it/s, loss=4.63e+4]\n",
            "epoch 58: 100%|██████████| 1366/1366 [00:20<00:00, 67.63it/s, loss=4.6e+4]\n",
            "epoch 59: 100%|██████████| 1366/1366 [00:20<00:00, 66.56it/s, loss=4.6e+4]\n",
            "epoch 60: 100%|██████████| 1366/1366 [00:19<00:00, 68.99it/s, loss=4.62e+4]\n",
            "epoch 61: 100%|██████████| 1366/1366 [00:19<00:00, 69.17it/s, loss=4.61e+4]\n",
            "epoch 62: 100%|██████████| 1366/1366 [00:19<00:00, 69.11it/s, loss=4.61e+4]\n",
            "epoch 63: 100%|██████████| 1366/1366 [00:20<00:00, 68.27it/s, loss=4.61e+4]\n",
            "epoch 64: 100%|██████████| 1366/1366 [00:20<00:00, 67.78it/s, loss=4.6e+4]\n",
            "epoch 65: 100%|██████████| 1366/1366 [00:20<00:00, 66.28it/s, loss=4.59e+4]\n",
            "epoch 66: 100%|██████████| 1366/1366 [00:20<00:00, 67.14it/s, loss=4.59e+4]\n",
            "epoch 67: 100%|██████████| 1366/1366 [00:20<00:00, 68.11it/s, loss=4.57e+4]\n",
            "epoch 68: 100%|██████████| 1366/1366 [00:20<00:00, 67.94it/s, loss=4.58e+4]\n",
            "epoch 69: 100%|██████████| 1366/1366 [00:20<00:00, 66.88it/s, loss=4.57e+4]\n",
            "epoch 70: 100%|██████████| 1366/1366 [00:20<00:00, 67.42it/s, loss=4.57e+4]\n",
            "epoch 71: 100%|██████████| 1366/1366 [00:20<00:00, 68.19it/s, loss=4.58e+4]\n",
            "epoch 72: 100%|██████████| 1366/1366 [00:20<00:00, 68.01it/s, loss=4.58e+4]\n",
            "epoch 73: 100%|██████████| 1366/1366 [00:20<00:00, 68.23it/s, loss=4.57e+4]\n",
            "epoch 74: 100%|██████████| 1366/1366 [00:20<00:00, 67.92it/s, loss=4.57e+4]\n",
            "epoch 75: 100%|██████████| 1366/1366 [00:22<00:00, 62.03it/s, loss=4.56e+4]\n",
            "epoch 76: 100%|██████████| 1366/1366 [00:19<00:00, 68.65it/s, loss=4.56e+4]\n",
            "epoch 77: 100%|██████████| 1366/1366 [00:20<00:00, 67.12it/s, loss=4.55e+4]\n",
            "epoch 78: 100%|██████████| 1366/1366 [00:21<00:00, 64.84it/s, loss=4.56e+4]\n",
            "epoch 79: 100%|██████████| 1366/1366 [00:20<00:00, 67.88it/s, loss=4.55e+4]\n",
            "epoch 80: 100%|██████████| 1366/1366 [00:19<00:00, 69.16it/s, loss=4.54e+4]\n",
            "epoch 81: 100%|██████████| 1366/1366 [00:20<00:00, 67.55it/s, loss=4.54e+4]\n",
            "epoch 82: 100%|██████████| 1366/1366 [00:20<00:00, 67.66it/s, loss=4.54e+4]\n",
            "epoch 83: 100%|██████████| 1366/1366 [00:20<00:00, 66.69it/s, loss=4.54e+4]\n",
            "epoch 84: 100%|██████████| 1366/1366 [00:20<00:00, 65.79it/s, loss=4.55e+4]\n",
            "epoch 85: 100%|██████████| 1366/1366 [00:20<00:00, 65.37it/s, loss=4.56e+4]\n",
            "epoch 86: 100%|██████████| 1366/1366 [00:20<00:00, 67.42it/s, loss=4.55e+4]\n",
            "epoch 87: 100%|██████████| 1366/1366 [00:20<00:00, 67.42it/s, loss=4.53e+4]\n",
            "epoch 88: 100%|██████████| 1366/1366 [00:20<00:00, 67.44it/s, loss=4.55e+4]\n",
            "epoch 89: 100%|██████████| 1366/1366 [00:20<00:00, 68.24it/s, loss=4.55e+4]\n",
            "epoch 90: 100%|██████████| 1366/1366 [00:20<00:00, 66.30it/s, loss=4.55e+4]\n",
            "epoch 91: 100%|██████████| 1366/1366 [00:20<00:00, 68.09it/s, loss=4.53e+4]\n",
            "epoch 92: 100%|██████████| 1366/1366 [00:21<00:00, 63.75it/s, loss=4.54e+4]\n",
            "epoch 93: 100%|██████████| 1366/1366 [00:20<00:00, 66.64it/s, loss=4.53e+4]\n",
            "epoch 94: 100%|██████████| 1366/1366 [00:20<00:00, 66.93it/s, loss=4.54e+4]\n",
            "epoch 95: 100%|██████████| 1366/1366 [00:20<00:00, 67.72it/s, loss=4.55e+4]\n",
            "epoch 96: 100%|██████████| 1366/1366 [00:20<00:00, 66.59it/s, loss=4.56e+4]\n",
            "epoch 97: 100%|██████████| 1366/1366 [00:21<00:00, 62.50it/s, loss=4.53e+4]\n",
            "epoch 98: 100%|██████████| 1366/1366 [00:21<00:00, 62.47it/s, loss=4.53e+4]\n",
            "epoch 99: 100%|██████████| 1366/1366 [00:20<00:00, 65.58it/s, loss=4.52e+4]\n",
            "epoch 100: 100%|██████████| 1366/1366 [00:20<00:00, 66.89it/s, loss=4.51e+4]\n"
          ]
        }
      ],
      "source": [
        "model = WideDeep(deeptabular=tab_mlp)\n",
        "\n",
        "trainer = Trainer(model, objective=\"rmse\", num_workers=0)\n",
        "\n",
        "trainer.fit(\n",
        "    X_tab=X_tab,\n",
        "    target=train_df['resale_price'].values,\n",
        "    n_epochs=100,\n",
        "    batch_size=64\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V46s-MdM0y5c"
      },
      "source": [
        "3.Report the test RMSE and the test R2 value that you obtained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KAhAgvMC07g6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74ab4077-4c50-476f-d424-d3225f6dc754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "predict: 100%|██████████| 1128/1128 [00:10<00:00, 104.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE 100923.36314600529\n",
            "R Squared 0.6441376209259033\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Inference\n",
        "X_tab_test = tab_preprocessor.transform(test_df)\n",
        "predictions = trainer.predict(X_tab=X_tab_test, batch_size=64)\n",
        "\n",
        "y_labels = test_df['resale_price'].values.tolist()\n",
        "mse = np.sqrt(mean_squared_error(y_labels, predictions))\n",
        "r_squared = r2_score(y_labels, predictions)\n",
        "\n",
        "print(\"RMSE\", mse)\n",
        "print(\"R Squared\", r_squared)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L77LuSvEWfhx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}