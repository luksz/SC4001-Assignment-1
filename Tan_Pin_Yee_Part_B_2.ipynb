{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFVxWZGJxprU"
   },
   "source": [
    "# Question B2 (10 marks)\n",
    "In Question B1, we used the Category Embedding model. This creates a feedforward neural network in which the categorical features get learnable embeddings. In this question, we will make use of a library called Pytorch-WideDeep. This library makes it easy to work with multimodal deep-learning problems combining images, text, and tables. We will just be utilizing the deeptabular component of this library through the TabMlp network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EycCozG06Duu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-widedeep\n",
      "  Using cached pytorch_widedeep-1.5.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas>=1.3.5 in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-widedeep) (2.2.1)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-widedeep) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.3 in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-widedeep) (1.12.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-widedeep) (1.4.1.post1)\n",
      "Requirement already satisfied: gensim in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-widedeep) (4.3.2)\n",
      "Collecting spacy (from pytorch-widedeep)\n",
      "  Using cached spacy-3.7.4-cp312-cp312-win_amd64.whl.metadata (27 kB)\n",
      "Collecting opencv-contrib-python (from pytorch-widedeep)\n",
      "  Using cached opencv_contrib_python-4.9.0.80-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting imutils (from pytorch-widedeep)\n",
      "  Using cached imutils-0.5.4-py3-none-any.whl\n",
      "Collecting tqdm (from pytorch-widedeep)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-widedeep) (2.2.1)\n",
      "Collecting torchvision>=0.15.0 (from pytorch-widedeep)\n",
      "  Using cached torchvision-0.17.1-cp312-cp312-win_amd64.whl.metadata (6.6 kB)\n",
      "Collecting einops (from pytorch-widedeep)\n",
      "  Using cached einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-widedeep) (1.16.0)\n",
      "Collecting torchmetrics (from pytorch-widedeep)\n",
      "  Using cached torchmetrics-1.3.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting pyarrow (from pytorch-widedeep)\n",
      "  Using cached pyarrow-15.0.1-cp312-cp312-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting fastparquet>=0.8.1 (from pytorch-widedeep)\n",
      "  Using cached fastparquet-2024.2.0-cp312-cp312-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting cramjam>=2.3 (from fastparquet>=0.8.1->pytorch-widedeep)\n",
      "  Using cached cramjam-2.8.2-cp312-none-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fastparquet>=0.8.1->pytorch-widedeep) (2024.2.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fastparquet>=0.8.1->pytorch-widedeep) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.3.5->pytorch-widedeep) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.3.5->pytorch-widedeep) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.3.5->pytorch-widedeep) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=1.0.2->pytorch-widedeep) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=1.0.2->pytorch-widedeep) (3.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.0.0->pytorch-widedeep) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.0.0->pytorch-widedeep) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.0.0->pytorch-widedeep) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.0.0->pytorch-widedeep) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.0.0->pytorch-widedeep) (3.1.3)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision>=0.15.0->pytorch-widedeep)\n",
      "  Using cached pillow-10.2.0-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim->pytorch-widedeep) (7.0.1)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy->pytorch-widedeep)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy->pytorch-widedeep)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy->pytorch-widedeep)\n",
      "  Using cached murmurhash-1.0.10-cp312-cp312-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy->pytorch-widedeep)\n",
      "  Using cached cymem-2.0.8-cp312-cp312-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy->pytorch-widedeep)\n",
      "  Using cached preshed-3.0.9-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy->pytorch-widedeep)\n",
      "  Using cached thinc-8.2.3-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy->pytorch-widedeep)\n",
      "  Using cached wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy->pytorch-widedeep)\n",
      "  Using cached srsly-2.4.8-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy->pytorch-widedeep)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy->pytorch-widedeep)\n",
      "  Using cached weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy->pytorch-widedeep)\n",
      "  Using cached typer-0.9.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim->pytorch-widedeep)\n",
      "  Using cached smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy->pytorch-widedeep) (2.31.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy->pytorch-widedeep)\n",
      "  Using cached pydantic-2.6.3-py3-none-any.whl.metadata (84 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy->pytorch-widedeep) (69.1.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy->pytorch-widedeep)\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->pytorch-widedeep) (0.4.6)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics->pytorch-widedeep)\n",
      "  Using cached lightning_utilities-0.10.1-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->pytorch-widedeep)\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.3 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->pytorch-widedeep)\n",
      "  Using cached pydantic_core-2.16.3-cp312-none-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.3.5->pytorch-widedeep) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->pytorch-widedeep) (2024.2.2)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy->pytorch-widedeep)\n",
      "  Using cached blis-0.7.11-cp312-cp312-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy->pytorch-widedeep)\n",
      "  Using cached confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting click<9.0.0,>=7.1.1 (from typer<0.10.0,>=0.3.0->spacy->pytorch-widedeep)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy->pytorch-widedeep)\n",
      "  Using cached cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=2.0.0->pytorch-widedeep) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\kyrive\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=2.0.0->pytorch-widedeep) (1.3.0)\n",
      "Using cached pytorch_widedeep-1.5.0-py3-none-any.whl (21.9 MB)\n",
      "Using cached fastparquet-2024.2.0-cp312-cp312-win_amd64.whl (669 kB)\n",
      "Using cached torchvision-0.17.1-cp312-cp312-win_amd64.whl (1.2 MB)\n",
      "Using cached einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "Using cached opencv_contrib_python-4.9.0.80-cp37-abi3-win_amd64.whl (45.3 MB)\n",
      "Using cached pyarrow-15.0.1-cp312-cp312-win_amd64.whl (25.3 MB)\n",
      "Using cached spacy-3.7.4-cp312-cp312-win_amd64.whl (11.7 MB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Using cached torchmetrics-1.3.1-py3-none-any.whl (840 kB)\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached cramjam-2.8.2-cp312-none-win_amd64.whl (1.6 MB)\n",
      "Using cached cymem-2.0.8-cp312-cp312-win_amd64.whl (39 kB)\n",
      "Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Using cached lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n",
      "Using cached murmurhash-1.0.10-cp312-cp312-win_amd64.whl (25 kB)\n",
      "Using cached pillow-10.2.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "Using cached preshed-3.0.9-cp312-cp312-win_amd64.whl (122 kB)\n",
      "Using cached pydantic-2.6.3-py3-none-any.whl (395 kB)\n",
      "Using cached pydantic_core-2.16.3-cp312-none-win_amd64.whl (1.9 MB)\n",
      "Using cached smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Using cached srsly-2.4.8-cp312-cp312-win_amd64.whl (478 kB)\n",
      "Using cached thinc-8.2.3-cp312-cp312-win_amd64.whl (1.4 MB)\n",
      "Using cached typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Using cached wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached blis-0.7.11-cp312-cp312-win_amd64.whl (6.6 MB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "Using cached confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: imutils, cymem, wasabi, tqdm, spacy-loggers, spacy-legacy, smart-open, pydantic-core, pyarrow, pillow, opencv-contrib-python, murmurhash, lightning-utilities, langcodes, einops, cramjam, cloudpathlib, click, catalogue, blis, annotated-types, typer, srsly, pydantic, preshed, torchvision, torchmetrics, fastparquet, confection, weasel, thinc, spacy, pytorch-widedeep\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 7.0.1\n",
      "    Uninstalling smart-open-7.0.1:\n",
      "      Successfully uninstalled smart-open-7.0.1\n",
      "Successfully installed annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 click-8.1.7 cloudpathlib-0.16.0 confection-0.1.4 cramjam-2.8.2 cymem-2.0.8 einops-0.7.0 fastparquet-2024.2.0 imutils-0.5.4 langcodes-3.3.0 lightning-utilities-0.10.1 murmurhash-1.0.10 opencv-contrib-python-4.9.0.80 pillow-10.2.0 preshed-3.0.9 pyarrow-15.0.1 pydantic-2.6.3 pydantic-core-2.16.3 pytorch-widedeep-1.5.0 smart-open-6.4.0 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 torchmetrics-1.3.1 torchvision-0.17.1 tqdm-4.66.2 typer-0.9.0 wasabi-1.1.2 weasel-0.3.4\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-widedeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lq0elU0J53Yo"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "import os\n",
    "\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pytorch_widedeep.preprocessing import TabPreprocessor\n",
    "from pytorch_widedeep.models import TabMlp, WideDeep\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.metrics import R2Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU3xdVpwzuLx"
   },
   "source": [
    "1.Divide the dataset (‘hdb_price_prediction.csv’) into train and test sets by using entries from the year 2020 and before as training data, and entries from 2021 and after as the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_oYG6lNIh7Mp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data :  [2017 2018 2019 2020]\n",
      "Testing data :  [2021 2022 2023]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "df_train = df[df[\"year\"] <= 2020]\n",
    "df_test = df[df[\"year\"] >= 2021]\n",
    "\n",
    "print(\"Training data : \",df_train[\"year\"].unique())\n",
    "print(\"Testing data : \",df_test[\"year\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_q9PoR50JAA"
   },
   "source": [
    "2.Refer to the documentation of Pytorch-WideDeep and perform the following tasks:\n",
    "https://pytorch-widedeep.readthedocs.io/en/latest/index.html\n",
    "* Use [**TabPreprocessor**](https://pytorch-widedeep.readthedocs.io/en/latest/examples/01_preprocessors_and_utils.html#2-tabpreprocessor) to create the deeptabular component using the continuous\n",
    "features and the categorical features. Use this component to transform the training dataset.\n",
    "* Create the [**TabMlp**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/model_components.html#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp) model with 2 linear layers in the MLP, with 200 and 100 neurons respectively.\n",
    "* Create a [**Trainer**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/trainer.html#pytorch_widedeep.training.Trainer) for the training of the created TabMlp model with the root mean squared error (RMSE) cost function. Train the model for 100 epochs using this trainer, keeping a batch size of 64. (Note: set the *num_workers* parameter to 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZBY1iqUXtYWn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KYRIVE\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_widedeep\\preprocessing\\tab_preprocessor.py:360: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n",
      "epoch 1: 100%|██████████████████████████████| 1366/1366 [00:04<00:00, 301.87it/s, loss=1.8e+5, metrics={'r2': -1.1555}]\n",
      "epoch 2: 100%|██████████████████████████████| 1366/1366 [00:04<00:00, 299.49it/s, loss=9.99e+4, metrics={'r2': 0.4867}]\n",
      "epoch 3: 100%|██████████████████████████████| 1366/1366 [00:04<00:00, 299.28it/s, loss=7.99e+4, metrics={'r2': 0.6807}]\n",
      "epoch 4: 100%|██████████████████████████████| 1366/1366 [00:04<00:00, 297.29it/s, loss=6.54e+4, metrics={'r2': 0.8006}]\n",
      "epoch 5: 100%|██████████████████████████████| 1366/1366 [00:04<00:00, 303.09it/s, loss=5.97e+4, metrics={'r2': 0.8377}]\n",
      "epoch 6: 100%|██████████████████████████████| 1366/1366 [00:04<00:00, 305.28it/s, loss=5.72e+4, metrics={'r2': 0.8524}]\n",
      "epoch 7: 100%|██████████████████████████████| 1366/1366 [00:04<00:00, 306.72it/s, loss=5.57e+4, metrics={'r2': 0.8602}]\n",
      "epoch 8: 100%|██████████████████████████████| 1366/1366 [00:04<00:00, 304.07it/s, loss=5.45e+4, metrics={'r2': 0.8667}]\n",
      "epoch 9: 100%|██████████████████████████████| 1366/1366 [00:04<00:00, 303.62it/s, loss=5.35e+4, metrics={'r2': 0.8719}]\n",
      "epoch 10: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 297.44it/s, loss=5.25e+4, metrics={'r2': 0.8762}]\n",
      "epoch 11: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 303.90it/s, loss=5.15e+4, metrics={'r2': 0.8812}]\n",
      "epoch 12: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 304.70it/s, loss=5.07e+4, metrics={'r2': 0.8848}]\n",
      "epoch 13: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 314.33it/s, loss=5.02e+4, metrics={'r2': 0.8874}]\n",
      "epoch 14: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 315.59it/s, loss=4.95e+4, metrics={'r2': 0.8906}]\n",
      "epoch 15: 100%|██████████████████████████████| 1366/1366 [00:04<00:00, 315.06it/s, loss=4.9e+4, metrics={'r2': 0.8928}]\n",
      "epoch 16: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 315.97it/s, loss=4.85e+4, metrics={'r2': 0.8948}]\n",
      "epoch 17: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 316.70it/s, loss=4.83e+4, metrics={'r2': 0.8956}]\n",
      "epoch 18: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 315.84it/s, loss=4.81e+4, metrics={'r2': 0.8965}]\n",
      "epoch 19: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 314.61it/s, loss=4.79e+4, metrics={'r2': 0.8975}]\n",
      "epoch 20: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 309.43it/s, loss=4.77e+4, metrics={'r2': 0.8982}]\n",
      "epoch 21: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 312.84it/s, loss=4.76e+4, metrics={'r2': 0.8988}]\n",
      "epoch 22: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 317.20it/s, loss=4.73e+4, metrics={'r2': 0.8997}]\n",
      "epoch 23: 100%|██████████████████████████████| 1366/1366 [00:04<00:00, 319.00it/s, loss=4.7e+4, metrics={'r2': 0.9012}]\n",
      "epoch 24: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 317.85it/s, loss=4.72e+4, metrics={'r2': 0.9004}]\n",
      "epoch 25: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 318.34it/s, loss=4.72e+4, metrics={'r2': 0.9004}]\n",
      "epoch 26: 100%|███████████████████████████████| 1366/1366 [00:04<00:00, 320.25it/s, loss=4.7e+4, metrics={'r2': 0.901}]\n",
      "epoch 27: 100%|██████████████████████████████| 1366/1366 [00:04<00:00, 318.13it/s, loss=4.7e+4, metrics={'r2': 0.9012}]\n",
      "epoch 28: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 318.79it/s, loss=4.69e+4, metrics={'r2': 0.9015}]\n",
      "epoch 29: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 317.33it/s, loss=4.67e+4, metrics={'r2': 0.9025}]\n",
      "epoch 30: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 318.92it/s, loss=4.66e+4, metrics={'r2': 0.9029}]\n",
      "epoch 31: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 317.08it/s, loss=4.68e+4, metrics={'r2': 0.9018}]\n",
      "epoch 32: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 320.01it/s, loss=4.66e+4, metrics={'r2': 0.9027}]\n",
      "epoch 33: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 320.75it/s, loss=4.67e+4, metrics={'r2': 0.9023}]\n",
      "epoch 34: 100%|██████████████████████████████| 1366/1366 [00:04<00:00, 318.21it/s, loss=4.65e+4, metrics={'r2': 0.903}]\n",
      "epoch 35: 100%|██████████████████████████████| 1366/1366 [00:04<00:00, 318.68it/s, loss=4.63e+4, metrics={'r2': 0.904}]\n",
      "epoch 36: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 318.72it/s, loss=4.63e+4, metrics={'r2': 0.9039}]\n",
      "epoch 37: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 318.84it/s, loss=4.64e+4, metrics={'r2': 0.9036}]\n",
      "epoch 38: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 318.36it/s, loss=4.64e+4, metrics={'r2': 0.9035}]\n",
      "epoch 39: 100%|██████████████████████████████| 1366/1366 [00:04<00:00, 317.12it/s, loss=4.63e+4, metrics={'r2': 0.904}]\n",
      "epoch 40: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 318.14it/s, loss=4.64e+4, metrics={'r2': 0.9035}]\n",
      "epoch 41: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 319.50it/s, loss=4.63e+4, metrics={'r2': 0.9039}]\n",
      "epoch 42: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 318.33it/s, loss=4.61e+4, metrics={'r2': 0.9047}]\n",
      "epoch 43: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 318.75it/s, loss=4.62e+4, metrics={'r2': 0.9042}]\n",
      "epoch 44: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 319.07it/s, loss=4.61e+4, metrics={'r2': 0.9048}]\n",
      "epoch 45: 100%|██████████████████████████████| 1366/1366 [00:04<00:00, 317.47it/s, loss=4.6e+4, metrics={'r2': 0.9054}]\n",
      "epoch 46: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 317.67it/s, loss=4.61e+4, metrics={'r2': 0.9047}]\n",
      "epoch 47: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 317.24it/s, loss=4.61e+4, metrics={'r2': 0.9049}]\n",
      "epoch 48: 100%|██████████████████████████████| 1366/1366 [00:04<00:00, 317.55it/s, loss=4.6e+4, metrics={'r2': 0.9049}]\n",
      "epoch 49: 100%|██████████████████████████████| 1366/1366 [00:04<00:00, 318.19it/s, loss=4.6e+4, metrics={'r2': 0.9051}]\n",
      "epoch 50: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 319.15it/s, loss=4.59e+4, metrics={'r2': 0.9058}]\n",
      "epoch 51: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 318.52it/s, loss=4.58e+4, metrics={'r2': 0.9058}]\n",
      "epoch 52: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 317.69it/s, loss=4.58e+4, metrics={'r2': 0.9061}]\n",
      "epoch 53: 100%|██████████████████████████████| 1366/1366 [00:04<00:00, 316.27it/s, loss=4.58e+4, metrics={'r2': 0.906}]\n",
      "epoch 54: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 316.81it/s, loss=4.58e+4, metrics={'r2': 0.9062}]\n",
      "epoch 55: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 316.89it/s, loss=4.56e+4, metrics={'r2': 0.9066}]\n",
      "epoch 56: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 316.92it/s, loss=4.57e+4, metrics={'r2': 0.9062}]\n",
      "epoch 57: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 316.48it/s, loss=4.56e+4, metrics={'r2': 0.9066}]\n",
      "epoch 58: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 315.31it/s, loss=4.56e+4, metrics={'r2': 0.9066}]\n",
      "epoch 59: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 309.87it/s, loss=4.56e+4, metrics={'r2': 0.9068}]\n",
      "epoch 60: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 308.96it/s, loss=4.57e+4, metrics={'r2': 0.9063}]\n",
      "epoch 61: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 311.44it/s, loss=4.56e+4, metrics={'r2': 0.9065}]\n",
      "epoch 62: 100%|██████████████████████████████| 1366/1366 [00:04<00:00, 309.77it/s, loss=4.57e+4, metrics={'r2': 0.906}]\n",
      "epoch 63: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 310.68it/s, loss=4.55e+4, metrics={'r2': 0.9068}]\n",
      "epoch 64: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 310.26it/s, loss=4.56e+4, metrics={'r2': 0.9067}]\n",
      "epoch 65: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 312.01it/s, loss=4.53e+4, metrics={'r2': 0.9075}]\n",
      "epoch 66: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 309.89it/s, loss=4.55e+4, metrics={'r2': 0.9069}]\n",
      "epoch 67: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 310.31it/s, loss=4.54e+4, metrics={'r2': 0.9073}]\n",
      "epoch 68: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 311.01it/s, loss=4.55e+4, metrics={'r2': 0.9068}]\n",
      "epoch 69: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 310.48it/s, loss=4.54e+4, metrics={'r2': 0.9077}]\n",
      "epoch 70: 100%|██████████████████████████████| 1366/1366 [00:04<00:00, 311.59it/s, loss=4.52e+4, metrics={'r2': 0.908}]\n",
      "epoch 71: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 311.16it/s, loss=4.51e+4, metrics={'r2': 0.9083}]\n",
      "epoch 72: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 311.33it/s, loss=4.54e+4, metrics={'r2': 0.9075}]\n",
      "epoch 73: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 310.60it/s, loss=4.54e+4, metrics={'r2': 0.9074}]\n",
      "epoch 74: 100%|██████████████████████████████| 1366/1366 [00:04<00:00, 310.74it/s, loss=4.52e+4, metrics={'r2': 0.908}]\n",
      "epoch 75: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 311.53it/s, loss=4.53e+4, metrics={'r2': 0.9078}]\n",
      "epoch 76: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 309.22it/s, loss=4.51e+4, metrics={'r2': 0.9082}]\n",
      "epoch 77: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 310.40it/s, loss=4.51e+4, metrics={'r2': 0.9082}]\n",
      "epoch 78: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 311.24it/s, loss=4.51e+4, metrics={'r2': 0.9083}]\n",
      "epoch 79: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 311.85it/s, loss=4.52e+4, metrics={'r2': 0.9079}]\n",
      "epoch 80: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 310.80it/s, loss=4.51e+4, metrics={'r2': 0.9083}]\n",
      "epoch 81: 100%|██████████████████████████████| 1366/1366 [00:04<00:00, 310.59it/s, loss=4.5e+4, metrics={'r2': 0.9087}]\n",
      "epoch 82: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 308.74it/s, loss=4.49e+4, metrics={'r2': 0.9093}]\n",
      "epoch 83: 100%|███████████████████████████████| 1366/1366 [00:04<00:00, 305.82it/s, loss=4.5e+4, metrics={'r2': 0.909}]\n",
      "epoch 84: 100%|██████████████████████████████| 1366/1366 [00:04<00:00, 308.29it/s, loss=4.5e+4, metrics={'r2': 0.9088}]\n",
      "epoch 85: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 308.27it/s, loss=4.49e+4, metrics={'r2': 0.9091}]\n",
      "epoch 86: 100%|██████████████████████████████| 1366/1366 [00:04<00:00, 312.68it/s, loss=4.5e+4, metrics={'r2': 0.9087}]\n",
      "epoch 87: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 310.97it/s, loss=4.49e+4, metrics={'r2': 0.9091}]\n",
      "epoch 88: 100%|██████████████████████████████| 1366/1366 [00:04<00:00, 311.33it/s, loss=4.5e+4, metrics={'r2': 0.9087}]\n",
      "epoch 89: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 311.12it/s, loss=4.48e+4, metrics={'r2': 0.9096}]\n",
      "epoch 90: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 310.28it/s, loss=4.51e+4, metrics={'r2': 0.9087}]\n",
      "epoch 91: 100%|██████████████████████████████| 1366/1366 [00:04<00:00, 310.71it/s, loss=4.49e+4, metrics={'r2': 0.909}]\n",
      "epoch 92: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 311.53it/s, loss=4.46e+4, metrics={'r2': 0.9102}]\n",
      "epoch 93: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 311.48it/s, loss=4.47e+4, metrics={'r2': 0.9102}]\n",
      "epoch 94: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 310.57it/s, loss=4.48e+4, metrics={'r2': 0.9094}]\n",
      "epoch 95: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 311.00it/s, loss=4.48e+4, metrics={'r2': 0.9097}]\n",
      "epoch 96: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 311.66it/s, loss=4.47e+4, metrics={'r2': 0.9098}]\n",
      "epoch 97: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 310.84it/s, loss=4.46e+4, metrics={'r2': 0.9105}]\n",
      "epoch 98: 100%|██████████████████████████████| 1366/1366 [00:04<00:00, 310.54it/s, loss=4.45e+4, metrics={'r2': 0.911}]\n",
      "epoch 99: 100%|█████████████████████████████| 1366/1366 [00:04<00:00, 310.66it/s, loss=4.46e+4, metrics={'r2': 0.9105}]\n",
      "epoch 100: 100%|████████████████████████████| 1366/1366 [00:04<00:00, 312.14it/s, loss=4.46e+4, metrics={'r2': 0.9104}]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE & RESULT HERE\n",
    "\n",
    "cat_embed_cols=[\"month\", \"town\", \"flat_model_type\", \"storey_range\"]\n",
    "continuous_cols=[\"dist_to_nearest_stn\", \"dist_to_dhoby\", \"degree_centrality\", \"eigenvector_centrality\", \"remaining_lease_years\", \"floor_area_sqm\"]\n",
    "\n",
    "\n",
    "#Use TabPreprocessor to create the deeptabular component using the continuous features and the categorical features. Use this component to transform the training dataset.\n",
    "tab_preprocessor = TabPreprocessor(\n",
    "    cat_embed_cols=cat_embed_cols,\n",
    "    continuous_cols=continuous_cols\n",
    "    #cols_to_scale=['resale_price'],  # or scale=True or cols_to_scale=continuous_cols\n",
    ")\n",
    "\n",
    "#For Training\n",
    "X_tab = tab_preprocessor.fit_transform(df_train) # x_train\n",
    "target = df_train['resale_price'].values # y_train\n",
    "\n",
    "# Create the TabMlp model with 2 linear layers in the MLP, with 200 and 100 neurons respectively.\n",
    "model = TabMlp(\n",
    "    column_idx=tab_preprocessor.column_idx,  \n",
    "    cat_embed_input=tab_preprocessor.cat_embed_input, \n",
    "    continuous_cols=continuous_cols, \n",
    "    #mlp_activation='relu', \n",
    "    #mlp_dropout=0.1,\n",
    "    mlp_hidden_dims=[200, 100] \n",
    "     \n",
    ")\n",
    "\n",
    "# Create a Trainer for the training of the created TabMlp model with the root mean squared error (RMSE) cost function. \n",
    "wide_deep_model = WideDeep(deeptabular=model) # Combine the TabMlp model with any other models you want to use\n",
    "Trainer_ = Trainer(\n",
    "    wide_deep_model, \n",
    "    objective=\"root_mean_squared_error\", \n",
    "    metrics=[R2Score], \n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Train the model for 100 epochs using this trainer, keeping a batch size of 64. (Note: set the num_workers parameter to 0.)\n",
    "Trainer_.fit(X_tab=X_tab, target=target, n_epochs=100, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V46s-MdM0y5c"
   },
   "source": [
    "3.Report the test RMSE and the test R2 value that you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KAhAgvMC07g6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|████████████████████████████████████████████████████████████████████| 1128/1128 [00:01<00:00, 812.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE :  102324.94465455868\n",
      "R2 :  0.6341849515710125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE & RESULT HERE\n",
    "import math\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "x_test = tab_preprocessor.transform(df_test)\n",
    "y_test = df_test['resale_price'].values\n",
    "\n",
    "predictions = Trainer_.predict(X_tab=x_test,batch_size=64)\n",
    "\n",
    "print(\"RMSE : \", math.sqrt(mean_squared_error(df_test['resale_price'], predictions)))\n",
    "print(\"R2 : \", r2_score(df_test['resale_price'], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
